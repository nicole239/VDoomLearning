{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Learning - Proyecto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from random import sample, randint, random\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import skimage.color, skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "# ⚡ PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 🏋️‍♀️ Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# ⚡ 🤝 🏋️‍♀️\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1982895900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1982895900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-learning settings\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "epochs = 20\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "# NN learning settings\n",
    "batch_size = 64\n",
    "\n",
    "# Training regime\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "# Other parameters\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"./model-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "# Configuration file path\n",
    "#config_file_path = \"../scenarios/simpler_basic.cfg\"\n",
    "config_file_path = \"../scenarios/deadly_corridor.cfg\"\n",
    "#config_file_path = \"../../scenarios/basic.cfg\"\n",
    "\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts and down-samples the input image\n",
    "def preprocess(img):\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img.reshape([1, 1, resolution[0], resolution[1]])\n",
    "    img = torch.from_numpy(img)\n",
    "    img = Variable(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distance(x1,y1,x2,y2):\n",
    "    return math.sqrt(math.pow(x1-x2,2) + math.pow(y1-y2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reproducir juego aprendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        channels = 1\n",
    "        state_shape = (capacity, channels, resolution[0], resolution[1])\n",
    "        self.s1 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.s2 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.a = np.zeros(capacity, dtype=np.int32)\n",
    "        self.r = np.zeros(capacity, dtype=np.float32)\n",
    "        self.isterminal = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    def add_transition(self, s1, action, s2, isterminal, reward):\n",
    "        self.s1[self.pos, 0, :, :] = s1\n",
    "        self.a[self.pos] = action\n",
    "        if not isterminal:\n",
    "            self.s2[self.pos, 0, :, :] = s2\n",
    "        self.isterminal[self.pos] = isterminal\n",
    "        self.r[self.pos] = reward\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        i = sample(range(0, self.size), sample_size)\n",
    "        return self.s1[i], self.a[i], self.r[i], self.isterminal[i], self.s2[i] \n",
    "\n",
    "    def clean_memory(self):\n",
    "        self.pos = 0\n",
    "        self.size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable Dataset containing the ReplayMemory\n",
    "    which will be updated with new experiences during training\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayMemory, sample_size: int = batch_size) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.get_sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardVariables():\n",
    "    def __init__(self):\n",
    "        self.distance = 1312\n",
    "        self.ammo = 55\n",
    "        self.hits = 0\n",
    "        self.health = 100\n",
    "        self.goal_X = 1312\n",
    "        self.goal_Y = 0\n",
    "        self.total_reward = 0        \n",
    "\n",
    "    def update(self, game):\n",
    "        reward = 0\n",
    "\n",
    "        player_x = game.get_game_variable(GameVariable.POSITION_X)\n",
    "        player_y = game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        d = distance(self.goal_X,self.goal_Y,player_x,player_y )\n",
    "\n",
    "        reward += (self.distance - d )*10\n",
    "        self.distance = d\n",
    "        \n",
    "        hits = game.get_game_variable(GameVariable.KILLCOUNT) \n",
    "        delta_hits = hits - self.hits\n",
    "        reward += delta_hits * 100 \n",
    "        self.hits = hits\n",
    "\n",
    "        ammo = game.get_game_variable(GameVariable.AMMO4)\n",
    "        \n",
    "        if delta_hits == 0:\n",
    "            delta_ammo = ammo - self.ammo\n",
    "            reward += delta_ammo * 10\n",
    "        self.ammo = ammo\n",
    "        \n",
    "        health = game.get_game_variable(GameVariable.HEALTH) \n",
    "        reward += health - self.health\n",
    "        self.health = health\n",
    "\n",
    "        angle = game.get_game_variable(GameVariable.ANGLE)\n",
    "        if angle > 90 and angle < 270:\n",
    "            reward +=  abs(angle - 180) - 180\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modelo de Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=6, stride=3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, stride=2)\n",
    "        self.fc1 = nn.Linear(192, 128)\n",
    "        self.fc2 = nn.Linear(128, available_actions_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        print(\"Available actions count: \",available_actions_count)\n",
    "        self.vit = ViT(\n",
    "                    image_size = 45,\n",
    "                    patch_size = 5,\n",
    "                    num_classes = available_actions_count,\n",
    "                    dim = available_actions_count,\n",
    "                    depth = 2,\n",
    "                    heads = 2,\n",
    "                    mlp_dim = available_actions_count*2,\n",
    "                    dropout = 0.1,\n",
    "                    emb_dropout = 0.1,\n",
    "                    channels=1\n",
    "                    )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(pl.LightningModule):\n",
    "    def __init__(self, preprocess = True, use_original_model=False):\n",
    "        super().__init__()\n",
    "        self.lr      = learning_rate\n",
    "        self.preprocess = preprocess\n",
    "        # Create Doom instance\n",
    "        self.game = self.initialize_vizdoom(config_file_path)\n",
    "\n",
    "        # Action = which buttons are pressed\n",
    "        n = self.game.get_available_buttons_size()\n",
    "        self.actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "        # Create replay memory which will store the transitions\n",
    "        self.memory = ReplayMemory(capacity=replay_memory_size)\n",
    "\n",
    "        if use_original_model:            \n",
    "            self.net = VisionTransformer(len(self.actions))\n",
    "        else:\n",
    "            self.net = Net(len(self.actions))\n",
    "        \n",
    "    def initialize_vizdoom(self, config_file_path):\n",
    "        game = DoomGame()\n",
    "        game.load_config(config_file_path)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)        \n",
    "        game.set_depth_buffer_enabled(True)\n",
    "        game.set_labels_buffer_enabled(True)\n",
    "        game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        game.set_render_weapon(False)\n",
    "        game.set_render_hud(False)\n",
    "        game.init()\n",
    "        return game\n",
    "\n",
    "    def forward(self, x): \n",
    "        logits = self.net(x)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        return self(state).sample().item()\n",
    "\n",
    "    def get_state(self):\n",
    "        state = self.game.get_state()\n",
    "        if self.preprocess:            \n",
    "            frame = cv2.add(state.depth_buffer,state.labels_buffer)\n",
    "            state = preprocess(frame)\n",
    "        else:\n",
    "            state = preprocess(state.screen_buffer)\n",
    "        return state\n",
    "\n",
    "        \n",
    "    def loss(self,state, action, reward):       \n",
    "        logp = self(state).log_prob(action)\n",
    "        return -(logp * reward).mean()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        #Llena la memoria\n",
    "        self.memory.clean_memory()\n",
    "        self.game.new_episode()\n",
    "        variables = RewardVariables()\n",
    "        for learning_step in trange(learning_steps_per_epoch, leave=False):   \n",
    "            state = self.get_state()\n",
    "            action= self.get_action(state)\n",
    "            self.game.make_action(self.actions[action], frame_repeat)\n",
    "            reward = variables.update(self.game)\n",
    "            done = self.game.is_episode_finished()\n",
    "            next_state = self.get_state() if not done else None\n",
    "            # Remember the transition that was just experienced.\n",
    "            self.memory.add_transition(state, action, next_state, done, reward)\n",
    "            \n",
    "            if self.game.is_episode_finished():\n",
    "                score = self.game.get_total_reward()\n",
    "                self.log('train/score', score, on_epoch=True)    \n",
    "                self.log('train/reward', variables.total_reward, on_epoch=True)            \n",
    "                self.game.new_episode()  \n",
    "                variables = RewardVariables()      \n",
    "\n",
    "    def discount_reward(self,reward):\n",
    "        R = 0\n",
    "        rewards_with_discount=[]\n",
    "        for r in torch.flip(reward, [0]):\n",
    "            R = r + discount_factor * R\n",
    "            rewards_with_discount.insert(0,R)\n",
    "        \n",
    "        # Scale rewards\n",
    "        rewards_with_discount = torch.FloatTensor(rewards_with_discount)\n",
    "        rewards_with_discount = (rewards_with_discount - rewards_with_discount.mean()) / (rewards_with_discount.std() + np.finfo(np.float32).eps)\n",
    "        return rewards_with_discount\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        state, action, reward, _, _ = batch\n",
    "\n",
    "        # Discount future rewards back to the present       \n",
    "        reward = self.discount_reward(reward) \n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss(state,action,reward)        \n",
    "\n",
    "        # logging metrics we calculated by hand\n",
    "        self.log('train/loss', loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(model.parameters(), self.lr)\n",
    "\n",
    "    #paso de pruebas que se realizarán al finalizar todo el entrenamiento\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        print(\"Testing...\")\n",
    "\n",
    "    def on_validation_start(self):      \n",
    "        for _ in trange(test_episodes_per_epoch, leave=False):\n",
    "            self.game.new_episode()\n",
    "            variables = RewardVariables()\n",
    "            while not self.game.is_episode_finished():\n",
    "                state = self.get_state()\n",
    "                action= self.get_action(state)\n",
    "                self.game.make_action(self.actions[action], frame_repeat)\n",
    "                variables.update(self.game)\n",
    "\n",
    "            r = self.game.get_total_reward()\n",
    "            self.log(\"val/total_score\", r, on_step=True, on_epoch=True)\n",
    "            self.log(\"val/total_reward\", variables.total_reward, on_step=True, on_epoch=True)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        self.game.close()\n",
    "        \n",
    "        # Reinitialize the game with window visible\n",
    "        self.game.set_window_visible(True)\n",
    "        self.game.set_mode(Mode.PLAYER )\n",
    "        self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.init()\n",
    "\n",
    "        btns = self.game.get_available_buttons()\n",
    "        print(btns)\n",
    "        for _ in range(episodes_to_watch):\n",
    "            self.game.new_episode()\n",
    "            variables = RewardVariables()\n",
    "            while not self.game.is_episode_finished():\n",
    "                state = self.get_state()\n",
    "                state2 = self.game.get_state()\n",
    "                action = self.get_action(state)\n",
    "                \n",
    "\n",
    "                depth = state2.depth_buffer\n",
    "                label = state2.labels_buffer\n",
    "\n",
    "                combination = cv2.add(depth,label)\n",
    "                cv2.imshow('ViZDoom combination Buffer', combination)\n",
    "\n",
    "\n",
    "                cv2.waitKey(int(0.028 * 1000))\n",
    "\n",
    "                self.game.set_action(self.actions[action])\n",
    "                for _ in range(frame_repeat):\n",
    "                    self.game.advance_action()\n",
    "                variables.update(self.game)\n",
    "            # Sleep between episodes\n",
    "            sleep(1.0)\n",
    "            score = self.game.get_total_reward()\n",
    "            print(\"test/total_score\", score)\n",
    "            print(\"test/total_reward\",variables.total_reward)\n",
    "\n",
    "        torch.save(self.net, model_savefile)\n",
    "        cv2.destroyAllWindows()\n",
    "        self.game.close()\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences\"\"\"\n",
    "        self.hparams.episode_length = 200\n",
    "        self.hparams.batch_size = 16\n",
    "        dataset = RLDataset(self.memory, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(dataset=dataset,\n",
    "                                batch_size=self.hparams.batch_size,\n",
    "                                )\n",
    "        return dataloader        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Button.MOVE_LEFT, Button.MOVE_RIGHT, Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_BACKWARD, Button.TURN_LEFT, Button.TURN_RIGHT]\n",
      "test/total_score 293.29005432128906\n",
      "test/total_reward -670.8547524766074\n",
      "test/total_score 81.60090637207031\n",
      "test/total_reward 121.48779903341142\n",
      "test/total_score -65.11970520019531\n",
      "test/total_reward -280.2041334912258\n",
      "test/total_score -37.21434020996094\n",
      "test/total_reward -441.1290409570588\n",
      "test/total_score -109.65455627441406\n",
      "test/total_reward -531.0714446753675\n",
      "test/total_score 161.2923126220703\n",
      "test/total_reward -1082.9309006201406\n",
      "test/total_score -5.74847412109375\n",
      "test/total_reward -11.80517498740619\n",
      "test/total_score 179.3506622314453\n",
      "test/total_reward -522.054243565593\n",
      "test/total_score 232.18496704101562\n",
      "test/total_reward 172.0718525887611\n",
      "test/total_score -115.99314880371094\n",
      "test/total_reward -215.9935650530249\n"
     ]
    }
   ],
   "source": [
    "#model = PolicyModel(preprocess= False)\n",
    "model.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ruben/.netrc\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "!wandb login --relogin 70434303a5ed54f05105b62078db2447874ee020\n",
    "wandb_logger = WandbLogger(project=\"VDoomLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/teccr/VDoomLearning/runs/3pru9hn9\" target=\"_blank\">eternal-plant-32</a></strong> to <a href=\"https://wandb.ai/teccr/VDoomLearning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | Net  | 42.1 K\n",
      "------------------------------\n",
      "42.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.1 K    Total params\n",
      "0.168     Total estimated model params size (MB)\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 0it [00:00, ?it/s, loss=-0.00748, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 0it [00:00, ?it/s, loss=-0.00105, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 0it [00:00, ?it/s, loss=0.00116, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 0it [00:11, ?it/s, loss=0.00116, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 0it [00:00, ?it/s, loss=0.00174, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 0it [00:14, ?it/s, loss=0.00174, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 0it [00:00, ?it/s, loss=-0.00244, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 0it [00:12, ?it/s, loss=-0.00244, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 0it [00:00, ?it/s, loss=-0.00673, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 0it [00:11, ?it/s, loss=-0.00673, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 0it [00:00, ?it/s, loss=-0.00512, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 0it [00:11, ?it/s, loss=-0.00512, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: : 0it [00:00, ?it/s, loss=-0.000386, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: : 0it [00:19, ?it/s, loss=-0.000386, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: : 0it [00:00, ?it/s, loss=5.44e-05, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: : 0it [00:10, ?it/s, loss=5.44e-05, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 0it [00:00, ?it/s, loss=-0.00216, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 0it [00:00, ?it/s, loss=0.00156, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 0it [00:13, ?it/s, loss=0.00156, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: : 0it [00:00, ?it/s, loss=0.000786, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: : 0it [00:13, ?it/s, loss=0.000786, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: : 0it [00:00, ?it/s, loss=-0.00183, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: : 0it [00:17, ?it/s, loss=-0.00183, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: : 0it [00:00, ?it/s, loss=-0.00161, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: : 0it [00:10, ?it/s, loss=-0.00161, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: : 0it [00:00, ?it/s, loss=-0.00215, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: : 0it [00:13, ?it/s, loss=-0.00215, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: : 0it [00:00, ?it/s, loss=-0.00289, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: : 0it [00:10, ?it/s, loss=-0.00289, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: : 0it [00:00, ?it/s, loss=-0.0042, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: : 0it [00:14, ?it/s, loss=-0.0042, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: : 0it [00:00, ?it/s, loss=0.00193, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: : 0it [00:17, ?it/s, loss=0.00193, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: : 0it [00:00, ?it/s, loss=-0.000308, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: : 0it [00:13, ?it/s, loss=-0.000308, v_num=9hn9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: : 13it [04:09, 19.16s/it, loss=0.00438, v_num=9hn9]\n",
      "[Button.MOVE_LEFT, Button.MOVE_RIGHT, Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_BACKWARD, Button.TURN_LEFT, Button.TURN_RIGHT]\n",
      "test/total_score 21.379608154296875\n",
      "test/total_reward 1000.9480308344368\n",
      "test/total_score 404.69969177246094\n",
      "test/total_reward 3348.458272518291\n",
      "test/total_score -0.4805450439453125\n",
      "test/total_reward -621.0221596449987\n",
      "test/total_score -23.177780151367188\n",
      "test/total_reward -642.1544120383805\n",
      "test/total_score 321.6117706298828\n",
      "test/total_reward -6213.700967073447\n",
      "test/total_score 238.41354370117188\n",
      "test/total_reward -7453.206711159541\n",
      "test/total_score 392.94752502441406\n",
      "test/total_reward -120.97170546284406\n",
      "test/total_score -43.489715576171875\n",
      "test/total_reward -1783.4022773512952\n",
      "test/total_score 249.03634643554688\n",
      "test/total_reward 22.714089784852376\n",
      "test/total_score 589.9055786132812\n",
      "test/total_reward 5819.2179035993395\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15364... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588df1386f504557a8cc26596eaefa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/loss_epoch</td><td>▁▅▆▄▃▄▂▅▅▄▆▆▄▄▂▄▃▆▄█</td></tr><tr><td>train/loss_step</td><td>▅▃▃▂▆▁▂▃▁▂▄▃▁▃▄▃▂▇▃▂▃▄▃▄▃█</td></tr><tr><td>train/reward</td><td>▅█▆█▆▁▇█▄▄▆▆▆▆▆▄▇▆▃▂</td></tr><tr><td>train/score</td><td>▄▆▃▅▃▄▇▃▁▂▂▄▄▆▃▆█▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/loss_epoch</td><td>0.00739</td></tr><tr><td>train/loss_step</td><td>0.04326</td></tr><tr><td>train/reward</td><td>-2856.33667</td></tr><tr><td>train/score</td><td>164.5106</td></tr><tr><td>trainer/global_step</td><td>259</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eternal-plant-32</strong>: <a href=\"https://wandb.ai/teccr/VDoomLearning/runs/3pru9hn9\" target=\"_blank\">https://wandb.ai/teccr/VDoomLearning/runs/3pru9hn9</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211121_211700-3pru9hn9\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger = wandb_logger,    # W&B integration\n",
    "    log_every_n_steps = 10,   # set the logging frequency\n",
    "    gpus = 0,                # use all GPUs\n",
    "    max_epochs = 20,           # number of epochs\n",
    "    deterministic = True,     # keep it deterministic\n",
    "    default_root_dir = \"./\"\n",
    ")\n",
    "\n",
    "model = PolicyModel(preprocess=True,use_original_model=False)\n",
    "trainer.fit(model)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimento 1  - B : Modelo base con preprocessing\n",
    "# 10 epochs\n",
    "\n",
    "#Experimento A: 1xiwwsve\n",
    "\n",
    "#model = PolicyModel.load_from_checkpoint(\".\\VDoomLearning\\8okrjet9\\checkpoints\\epoch=9-step=129.ckpt\")\n",
    "#trainer.fit(model)\n",
    "\n",
    "# 20 epochs\n",
    "model = PolicyModel.load_from_checkpoint(\".\\VDoomLearning\\2xw64o7u\\checkpoints\\epoch=19-step=259.ckpt\")\n",
    "trainer.fit(model)\n",
    "\n",
    "#Experimento 2: Nuevo modelo ViT\n",
    "#20 epochs\n",
    "model = PolicyModel.load_from_checkpoint(\".\\VDoomLearning\\2xukrzpg\\checkpoints\\epoch=19-step=259.ckpt\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Button.MOVE_LEFT, Button.MOVE_RIGHT, Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_BACKWARD, Button.TURN_LEFT, Button.TURN_RIGHT]\n",
      "test/total_score -32.399810791015625\n",
      "test/total_reward -702.0763478560989\n",
      "test/total_score -52.77935791015625\n",
      "test/total_reward -10.88044285881847\n",
      "test/total_score -79.49752807617188\n",
      "test/total_reward -267.97785675105047\n",
      "test/total_score -29.0035400390625\n",
      "test/total_reward -25.106505051959402\n",
      "test/total_score -56.83161926269531\n",
      "test/total_reward -48.93244871513866\n",
      "test/total_score 127.52180480957031\n",
      "test/total_reward -5498.480270982866\n",
      "test/total_score -115.35690307617188\n",
      "test/total_reward -197.45322167940708\n",
      "test/total_score 36.95648193359375\n",
      "test/total_reward -1618.5486809257545\n",
      "test/total_score 116.38494873046875\n",
      "test/total_reward 158.32152770620746\n",
      "test/total_score 57.40484619140625\n",
      "test/total_reward -1278.2596981869883\n"
     ]
    }
   ],
   "source": [
    "#model = PolicyModel.load_from_checkpoint(\".\\VDoomLearning\\2xukrzpg\\checkpoints\\epoch=19-step=259.ckpt\")\n",
    "model.on_train_end()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b31cb8f29005d79cbadb50c628970629d89324bc24395466016ed12a0acc794"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
