{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Learning - Proyecto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from random import sample, randint, random\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import skimage.color, skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "# ‚ö° PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# ‚ö° ü§ù üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 537065132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "537065132"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-learning settings\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "epochs = 20\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "# NN learning settings\n",
    "batch_size = 64\n",
    "\n",
    "# Training regime\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "# Other parameters\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"./model-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "# Configuration file path\n",
    "config_file_path = \"../scenarios/simpler_basic.cfg\"\n",
    "# config_file_path = \"../../scenarios/rocket_basic.cfg\"\n",
    "# config_file_path = \"../../scenarios/basic.cfg\"\n",
    "\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts and down-samples the input image\n",
    "def preprocess(img):\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img.reshape([1, 1, resolution[0], resolution[1]])\n",
    "    img = torch.from_numpy(img)\n",
    "    img = Variable(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reproducir juego aprendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        channels = 1\n",
    "        state_shape = (capacity, channels, resolution[0], resolution[1])\n",
    "        self.s1 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.s2 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.a = np.zeros(capacity, dtype=np.int32)\n",
    "        self.r = np.zeros(capacity, dtype=np.float32)\n",
    "        self.isterminal = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    def add_transition(self, s1, action, s2, isterminal, reward):\n",
    "        self.s1[self.pos, 0, :, :] = s1\n",
    "        self.a[self.pos] = action\n",
    "        if not isterminal:\n",
    "            self.s2[self.pos, 0, :, :] = s2\n",
    "        self.isterminal[self.pos] = isterminal\n",
    "        self.r[self.pos] = reward\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        i = sample(range(0, self.size), sample_size)\n",
    "        return self.s1[i], self.a[i], self.r[i], self.isterminal[i], self.s2[i] \n",
    "\n",
    "    def clean_memory(self):\n",
    "        self.pos = 0\n",
    "        self.size = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable Dataset containing the ReplayMemory\n",
    "    which will be updated with new experiences during training\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayMemory, sample_size: int = batch_size) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.get_sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modelo de Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=6, stride=3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, stride=2)\n",
    "        self.fc1 = nn.Linear(192, 128)\n",
    "        self.fc2 = nn.Linear(128, available_actions_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr      = learning_rate\n",
    "\n",
    "        # Create Doom instance\n",
    "        self.game = self.initialize_vizdoom(config_file_path)\n",
    "\n",
    "        # Action = which buttons are pressed\n",
    "        n = self.game.get_available_buttons_size()\n",
    "        self.actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "        # Create replay memory which will store the transitions\n",
    "        self.memory = ReplayMemory(capacity=replay_memory_size)\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_savefile)\n",
    "            self.net = torch.load(model_savefile)\n",
    "        else:\n",
    "            self.net = Net(len(self.actions))\n",
    "        \n",
    "    def initialize_vizdoom(self, config_file_path):\n",
    "        game = DoomGame()\n",
    "        game.load_config(config_file_path)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        game.init()\n",
    "        return game\n",
    "\n",
    "    def forward(self, x): \n",
    "        logits = self.net(x)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        return self(state).sample().item()\n",
    "        \n",
    "    def loss(self,state, action, reward):       \n",
    "        logp = self(state).log_prob(action)\n",
    "        return -(logp * reward).mean()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        #Llena la memoria\n",
    "        self.memory.clean_memory()\n",
    "        self.game.new_episode()\n",
    "        for learning_step in trange(learning_steps_per_epoch, leave=False):            \n",
    "            state = preprocess(self.game.get_state().screen_buffer)\n",
    "            action= self.get_action(state)\n",
    "            reward = self.game.make_action(self.actions[action], frame_repeat)\n",
    "            done = self.game.is_episode_finished()\n",
    "            next_state = preprocess(self.game.get_state().screen_buffer) if not done else None\n",
    "            # Remember the transition that was just experienced.\n",
    "            self.memory.add_transition(state, action, next_state, done, reward)\n",
    "            \n",
    "            if self.game.is_episode_finished():\n",
    "                score = self.game.get_total_reward()\n",
    "                self.log('train/score', score, on_epoch=True)                \n",
    "                self.game.new_episode()        \n",
    "\n",
    "    def discount_reward(self,reward):\n",
    "        R = 0\n",
    "        rewards_with_discount=[]\n",
    "        for r in torch.flip(reward, [0]):\n",
    "            R = r + discount_factor * R\n",
    "            rewards_with_discount.insert(0,R)\n",
    "        \n",
    "        # Scale rewards\n",
    "        rewards_with_discount = torch.FloatTensor(rewards_with_discount)\n",
    "        rewards_with_discount = (rewards_with_discount - rewards_with_discount.mean()) / (rewards_with_discount.std() + np.finfo(np.float32).eps)\n",
    "        return rewards_with_discount\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        state, action, reward, _, _ = batch\n",
    "\n",
    "        # Discount future rewards back to the present       \n",
    "        reward = self.discount_reward(reward) \n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss(state,action,reward)        \n",
    "\n",
    "        # logging metrics we calculated by hand\n",
    "        self.log('train/loss', loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(model.parameters(), self.lr)\n",
    "\n",
    "    #paso de pruebas que se realizar√°n al finalizar todo el entrenamiento\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        print(\"Hola\")\n",
    "        \n",
    "\n",
    "    def on_validation_start(self):      \n",
    "        for _ in trange(test_episodes_per_epoch, leave=False):\n",
    "            self.game.new_episode()\n",
    "            while not self.game.is_episode_finished():\n",
    "                state = preprocess(self.game.get_state().screen_buffer)\n",
    "                action= self.get_action(state)\n",
    "                self.game.make_action(self.actions[action], frame_repeat)\n",
    "\n",
    "            r = self.game.get_total_reward()\n",
    "            self.log(\"val/total_score\", r, on_step=True, on_epoch=True)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        self.game.close()\n",
    "\n",
    "        # Reinitialize the game with window visible\n",
    "        self.game.set_window_visible(True)\n",
    "        self.game.set_mode(Mode.ASYNC_PLAYER)\n",
    "        self.game.init()\n",
    "\n",
    "        for _ in range(episodes_to_watch):\n",
    "            self.game.new_episode()\n",
    "            while not self.game.is_episode_finished():\n",
    "                state = preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.get_action(state)\n",
    "\n",
    "                # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "                self.game.set_action(self.actions[action])\n",
    "                for _ in range(frame_repeat):\n",
    "                    self.game.advance_action()\n",
    "\n",
    "            # Sleep between episodes\n",
    "            sleep(1.0)\n",
    "            score = self.game.get_total_reward()\n",
    "            print(\"test/total_score\", score)\n",
    "\n",
    "        torch.save(self.net, model_savefile)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences\"\"\"\n",
    "        self.hparams.episode_length = 200\n",
    "        self.hparams.batch_size = 16\n",
    "        dataset = RLDataset(self.memory, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(dataset=dataset,\n",
    "                                batch_size=self.hparams.batch_size,\n",
    "                                )\n",
    "        return dataloader\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ruben/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin 70434303a5ed54f05105b62078db2447874ee020\n",
    "wandb_logger = WandbLogger(project=\"VDoomLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | Net  | 26.6 K\n",
      "------------------------------\n",
      "26.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.6 K    Total params\n",
      "0.106     Total estimated model params size (MB)\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:617: UserWarning: Checkpoint directory .\\VDoomLearning\\4cb1atpv\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 13it [03:06, 14.38s/it, loss=-0.000171, v_num=atpv]\n",
      "test/total_score 53.0\n",
      "test/total_score 94.0\n",
      "test/total_score 0.0\n",
      "test/total_score 70.0\n",
      "test/total_score 94.0\n",
      "test/total_score -128.0\n",
      "test/total_score -329.0\n",
      "test/total_score -155.0\n",
      "test/total_score -330.0\n",
      "test/total_score -356.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6052... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a66601af31842299a17af15d2fe50e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss_epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/loss_step</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/score</td><td>‚ñà‚ñÅ</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñà‚ñÅ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train/loss_epoch</td><td>0.00015</td></tr><tr><td>train/loss_step</td><td>0.00665</td></tr><tr><td>train/score</td><td>-100.21384</td></tr><tr><td>trainer/global_step</td><td>12</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">crimson-firefly-3</strong>: <a href=\"https://wandb.ai/teccr/VDoomLearning/runs/4cb1atpv\" target=\"_blank\">https://wandb.ai/teccr/VDoomLearning/runs/4cb1atpv</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211114_213714-4cb1atpv\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger = wandb_logger,    # W&B integration\n",
    "    log_every_n_steps = 10,   # set the logging frequency\n",
    "    gpus = 0,                # use all GPUs\n",
    "    max_epochs = 1,           # number of epochs\n",
    "    deterministic = True,     # keep it deterministic\n",
    "    default_root_dir = \"./\"\n",
    ")\n",
    "\n",
    "model = PolicyModel()\n",
    "trainer.fit(model)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Starting the training!\n",
      "\n",
      "Epoch 1\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 training episodes played.\n",
      "Results: mean: -95.6 +/- 186.4, min: -380.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -70.4 +/- 233.2, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 7.18 minutes\n",
      "\n",
      "Epoch 2\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 training episodes played.\n",
      "Results: mean: -68.4 +/- 169.9, min: -385.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -41.2 +/- 218.8, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 12.59 minutes\n",
      "\n",
      "Epoch 3\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 training episodes played.\n",
      "Results: mean: -87.9 +/- 188.4, min: -395.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -70.6 +/- 233.0, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 18.19 minutes\n",
      "\n",
      "Epoch 4\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 training episodes played.\n",
      "Results: mean: -65.5 +/- 182.0, min: -395.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -75.3 +/- 235.1, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 23.77 minutes\n",
      "\n",
      "Epoch 5\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 training episodes played.\n",
      "Results: mean: -51.3 +/- 181.4, min: -395.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -86.3 +/- 237.7, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 29.42 minutes\n",
      "\n",
      "Epoch 6\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 training episodes played.\n",
      "Results: mean: -56.8 +/- 190.5, min: -395.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -115.8 +/- 245.7, min: -410.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 35.32 minutes\n",
      "\n",
      "Epoch 7\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 training episodes played.\n",
      "Results: mean: -37.8 +/- 176.3, min: -400.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 4.0 +/- 173.0, min: -385.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 40.56 minutes\n",
      "\n",
      "Epoch 8\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 training episodes played.\n",
      "Results: mean: 36.5 +/- 90.1, min: -365.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 80.9 +/- 20.7, min: 0.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 45.40 minutes\n",
      "\n",
      "Epoch 9\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568 training episodes played.\n",
      "Results: mean: 56.6 +/- 51.8, min: -355.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 83.2 +/- 16.1, min: 38.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 50.27 minutes\n",
      "\n",
      "Epoch 10\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665 training episodes played.\n",
      "Results: mean: 64.5 +/- 46.8, min: -340.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 83.7 +/- 21.5, min: -50.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 55.53 minutes\n",
      "\n",
      "Epoch 11\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 training episodes played.\n",
      "Results: mean: 71.1 +/- 35.9, min: -237.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 79.8 +/- 22.5, min: -59.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 61.13 minutes\n",
      "\n",
      "Epoch 12\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808 training episodes played.\n",
      "Results: mean: 74.5 +/- 29.5, min: -93.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 80.1 +/- 24.7, min: -50.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 66.98 minutes\n",
      "\n",
      "Epoch 13\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964 training episodes played.\n",
      "Results: mean: 80.3 +/- 24.9, min: -112.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 83.0 +/- 18.3, min: -19.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 72.79 minutes\n",
      "\n",
      "Epoch 14\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 training episodes played.\n",
      "Results: mean: 81.2 +/- 21.7, min: -79.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 85.3 +/- 14.0, min: 38.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 78.52 minutes\n",
      "\n",
      "Epoch 15\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032 training episodes played.\n",
      "Results: mean: 82.2 +/- 20.7, min: -72.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 83.1 +/- 17.6, min: 37.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 84.35 minutes\n",
      "\n",
      "Epoch 16\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052 training episodes played.\n",
      "Results: mean: 82.9 +/- 17.2, min: -33.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 84.4 +/- 14.3, min: 45.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 90.27 minutes\n",
      "\n",
      "Epoch 17\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034 training episodes played.\n",
      "Results: mean: 82.6 +/- 17.1, min: -33.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 85.6 +/- 12.0, min: 45.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 96.07 minutes\n",
      "\n",
      "Epoch 18\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047 training episodes played.\n",
      "Results: mean: 83.1 +/- 16.9, min: -4.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 88.3 +/- 10.6, min: 52.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 101.93 minutes\n",
      "\n",
      "Epoch 19\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 training episodes played.\n",
      "Results: mean: 82.0 +/- 17.9, min: -28.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 85.0 +/- 13.0, min: 52.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 107.92 minutes\n",
      "\n",
      "Epoch 20\n",
      "-------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035 training episodes played.\n",
      "Results: mean: 82.7 +/- 17.9, min: -43.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 86.9 +/- 12.3, min: 45.0 max: 95.0\n",
      "Saving the network weigths to: ./model-doom.pth\n",
      "Total elapsed time: 113.88 minutes\n",
      "======================================\n",
      "Training finished. It's time to watch!\n",
      "Total score:  70.0\n",
      "Total score:  82.0\n",
      "Total score:  94.0\n",
      "Total score:  12.0\n",
      "Total score:  94.0\n",
      "Total score:  94.0\n",
      "Total score:  94.0\n",
      "Total score:  95.0\n",
      "Total score:  70.0\n",
      "Total score:  60.0\n"
     ]
    }
   ],
   "source": [
    "def learn(s1, target_q):\n",
    "    s1 = torch.from_numpy(s1)\n",
    "    target_q = torch.from_numpy(target_q)\n",
    "    s1, target_q = Variable(s1), Variable(target_q)\n",
    "    output = model(s1)\n",
    "    loss = criterion(output, target_q)\n",
    "    # compute gradient and do SGD step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def get_q_values(state):\n",
    "    state = torch.from_numpy(state)\n",
    "    state = Variable(state)\n",
    "    return model(state)\n",
    "\n",
    "def get_best_action(state):\n",
    "    q = get_q_values(state)\n",
    "    m, index = torch.max(q, 1)\n",
    "    action = index.data.numpy()[0]\n",
    "    return action\n",
    "\n",
    "\n",
    "def learn_from_memory():\n",
    "    \"\"\" Learns from a single transition (making use of replay memory).\n",
    "    s2 is ignored if s2_isterminal \"\"\"\n",
    "\n",
    "    # Get a random minibatch from the replay memory and learns from it.\n",
    "    if memory.size > batch_size:\n",
    "        s1, a, s2, isterminal, r = memory.get_sample(batch_size)\n",
    "\n",
    "        q = get_q_values(s2).data.numpy()\n",
    "        q2 = np.max(q, axis=1)\n",
    "        target_q = get_q_values(s1).data.numpy()\n",
    "        # target differs from q only for the selected action. The following means:\n",
    "        # target_Q(s,a) = r + gamma * max Q(s2,_) if isterminal else r\n",
    "        target_q[np.arange(target_q.shape[0]), a] = r + discount_factor * (1 - isterminal) * q2\n",
    "        learn(s1, target_q)\n",
    "\n",
    "\n",
    "def perform_learning_step(epoch):\n",
    "    \"\"\" Makes an action according to eps-greedy policy, observes the result\n",
    "    (next state, reward) and learns from the transition\"\"\"\n",
    "\n",
    "    def exploration_rate(epoch):\n",
    "        \"\"\"# Define exploration rate change over time\"\"\"\n",
    "        start_eps = 1.0\n",
    "        end_eps = 0.1\n",
    "        const_eps_epochs = 0.1 * epochs  # 10% of learning time\n",
    "        eps_decay_epochs = 0.6 * epochs  # 60% of learning time\n",
    "\n",
    "        if epoch < const_eps_epochs:\n",
    "            return start_eps\n",
    "        elif epoch < eps_decay_epochs:\n",
    "            # Linear decay\n",
    "            return start_eps - (epoch - const_eps_epochs) / (eps_decay_epochs - const_eps_epochs) * (start_eps - end_eps)\n",
    "        else:\n",
    "            return end_eps\n",
    "\n",
    "    s1 = preprocess(game.get_state().screen_buffer)\n",
    "\n",
    "    # With probability eps make a random action.\n",
    "    eps = exploration_rate(epoch)\n",
    "    if random() <= eps:\n",
    "        a = randint(0, len(actions) - 1)\n",
    "    else:\n",
    "        # Choose the best action according to the network.\n",
    "        s1 = s1.reshape([1, 1, resolution[0], resolution[1]])\n",
    "        a = get_best_action(s1)\n",
    "    reward = game.make_action(actions[a], frame_repeat)\n",
    "\n",
    "    isterminal = game.is_episode_finished()\n",
    "    s2 = preprocess(game.get_state().screen_buffer) if not isterminal else None\n",
    "\n",
    "    # Remember the transition that was just experienced.\n",
    "    memory.add_transition(s1, a, s2, isterminal, reward)\n",
    "\n",
    "    learn_from_memory()\n",
    "\n",
    "\n",
    "# Creates and initializes ViZDoom environment.\n",
    "def initialize_vizdoom(config_file_path):\n",
    "    print(\"Initializing doom...\")\n",
    "    game = DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    game.set_screen_format(ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "    return game\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create Doom instance\n",
    "    game = initialize_vizdoom(config_file_path)\n",
    "\n",
    "    # Action = which buttons are pressed\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    # Create replay memory which will store the transitions\n",
    "    memory = ReplayMemory(capacity=replay_memory_size)\n",
    "\n",
    "    if load_model:\n",
    "        print(\"Loading model from: \", model_savefile)\n",
    "        model = torch.load(model_savefile)\n",
    "    else:\n",
    "        model = Net(len(actions))\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "    print(\"Starting the training!\")\n",
    "    time_start = time()\n",
    "    if not skip_learning:\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nEpoch %d\\n-------\" % (epoch + 1))\n",
    "            train_episodes_finished = 0\n",
    "            train_scores = []\n",
    "\n",
    "            print(\"Training...\")\n",
    "            self.game.new_episode()\n",
    "            for learning_step in trange(learning_steps_per_epoch, leave=False):\n",
    "                perform_learning_step(epoch)\n",
    "                if game.is_episode_finished():\n",
    "                    score = game.get_total_reward()\n",
    "                    train_scores.append(score)\n",
    "                    game.new_episode()\n",
    "                    train_episodes_finished += 1\n",
    "\n",
    "            print(\"%d training episodes played.\" % train_episodes_finished)\n",
    "\n",
    "            train_scores = np.array(train_scores)\n",
    "\n",
    "            print(\"Results: mean: %.1f +/- %.1f,\" % (train_scores.mean(), train_scores.std()), \\\n",
    "                  \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "            print(\"\\nTesting...\")\n",
    "            test_episode = []\n",
    "            test_scores = []\n",
    "            for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "                game.new_episode()\n",
    "                while not game.is_episode_finished():\n",
    "                    state = preprocess(game.get_state().screen_buffer)\n",
    "                    state = state.reshape([1, 1, resolution[0], resolution[1]])\n",
    "                    best_action_index = get_best_action(state)\n",
    "\n",
    "                    game.make_action(actions[best_action_index], frame_repeat)\n",
    "                r = game.get_total_reward()\n",
    "                test_scores.append(r)\n",
    "\n",
    "            test_scores = np.array(test_scores)\n",
    "            print(\"Results: mean: %.1f +/- %.1f,\" % (\n",
    "                test_scores.mean(), test_scores.std()), \"min: %.1f\" % test_scores.min(),\n",
    "                  \"max: %.1f\" % test_scores.max())\n",
    "\n",
    "            print(\"Saving the network weigths to:\", model_savefile)\n",
    "            torch.save(model, model_savefile)\n",
    "\n",
    "            print(\"Total elapsed time: %.2f minutes\" % ((time() - time_start) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    print(\"======================================\")\n",
    "    print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "    # Reinitialize the game with window visible\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            state = state.reshape([1, 1, resolution[0], resolution[1]])\n",
    "            best_action_index = get_best_action(state)\n",
    "\n",
    "            # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "        # Sleep between episodes\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b31cb8f29005d79cbadb50c628970629d89324bc24395466016ed12a0acc794"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
