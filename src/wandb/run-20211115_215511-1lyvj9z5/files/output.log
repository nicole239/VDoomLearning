
Epoch 0: : 0it [00:00, ?it/s]
  | Name | Type | Params
------------------------------
0 | net  | Net  | 42.1 K
------------------------------
42.1 K    Trainable params
0         Non-trainable params
42.1 K    Total params
0.168     Total estimated model params size (MB)








































































































































                                                   Global seed set to 818201418
Epoch 0: : 0it [00:00, ?it/s]Loading model from:  ./model-doom.pth
Training: 0it [00:00, ?it/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\loggers\wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
  | Name | Type | Params
------------------------------
0 | net  | Net  | 42.1 K
------------------------------
42.1 K    Trainable params
0         Non-trainable params
42.1 K    Total params
0.168     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\trainer\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."












































































































































test/total_score -115.97731018066406
test/total_score -76.25360107421875
test/total_score -72.75440979003906
test/total_score -63.15742492675781
test/total_score 46.12141418457031
test/total_score -115.98200988769531
test/total_score 179.99761962890625
test/total_score 42.075531005859375
                                                   Global seed set to 818201418
wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\ruben/.netrc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Loading model from:  ./model-doom.pth
Epoch 0: : 0it [00:00, ?it/s]
  | Name | Type | Params
------------------------------
0 | net  | Net  | 42.1 K
------------------------------
42.1 K    Trainable params
0         Non-trainable params
42.1 K    Total params
0.168     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:617: UserWarning: Checkpoint directory .\VDoomLearning\1lyvj9z5\checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")


































































































































































































































































































































































































































































































































































































































Epoch 0: : 13it [20:26, 94.33s/it, loss=-4.61e-05, v_num=j9z5]
test/total_score -18.996963500976562
test/total_score -114.38267517089844
test/total_score -62.09393310546875
test/total_score -115.50827026367188
test/total_score -84.85446166992188
test/total_score -115.85304260253906
test/total_score -20.277420043945312
test/total_score -29.279510498046875
test/total_score 134.6260223388672
test/total_score 3.3242340087890625