
Epoch 0: : 0it [00:00, ?it/s]
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\trainer\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."
                                        wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\ruben/.netrc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores

Epoch 0: : 0it [00:00, ?it/s]Loading model from:  ./model-doom.pth
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\loggers\wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
Epoch 0: : 0it [00:00, ?it/s]
                                        wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\ruben/.netrc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
Epoch 0: : 0it [00:00, ?it/s]Loading model from:  ./model-doom.pth
Epoch 0: : 0it [00:00, ?it/s]
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)


                                        wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\ruben/.netrc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
Epoch 0: : 0it [00:00, ?it/s]Loading model from:  ./model-doom.pth
Epoch 0: : 0it [00:00, ?it/s]
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\loggers\wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\trainer\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."











































































































































































