
Epoch 0: : 0it [00:00, ?it/s]
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\trainer\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."

































































































                                                   GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\ruben/.netrc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\loggers\wandb.py:342: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  "There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse"
  | Name | Type | Params
------------------------------
0 | net  | Net  | 26.6 K
------------------------------
26.6 K    Trainable params
0         Non-trainable params
26.6 K    Total params
0.106     Total estimated model params size (MB)
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:617: UserWarning: Checkpoint directory .\VDoomLearning\4cb1atpv\checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
C:\Users\ruben\AppData\Local\Programs\Python\Python36\lib\site-packages\pytorch_lightning\trainer\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f"The dataloader, {name}, does not have many workers which may be a bottleneck."
  0%|          | 10/2000 [00:00<03:18, 10.02it/s]






























































































test/total_score 53.0
test/total_score 94.0
test/total_score 0.0
test/total_score 70.0
test/total_score 94.0
test/total_score -128.0
test/total_score -329.0
test/total_score -155.0
test/total_score -330.0
test/total_score -356.0